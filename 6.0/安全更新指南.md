# 游戏数据安全更新指南

> 本指南适用于游戏更新后，需要更新数据库但又要保留手动修正内容的情况

---

## 📋 更新流程总览

```
1. 备份当前数据    → backup_current_data.py
2. 获取最新名称    → simple_crawler.py + fetch_*_name.py
3. 对比找出新增项  → compare_names.py
4. 只爬取新增项    → 修改后的爬虫脚本
5. 检查并完善数据  → 手动处理
```

---

## 🛡️ 安全机制

### 🔄 智能覆盖策略
爬虫现在采用**智能覆盖**机制，确保你的手动修正不会丢失：

**覆盖规则：**
- ✅ **新数据有效** → 使用新数据覆盖
- ❌ **新数据为空** → 保留原有数据（包括手动修正）
- 🔄 **智能合并** → 只更新有效字段，保留其他字段

**具体逻辑：**
```python
# 描述：只有新描述不为空时才覆盖
if new_description.strip():
    merged['description'] = new_description

# 图标：只有新图标下载成功时才覆盖  
if new_icon_downloaded_successfully:
    merged['icon'] = new_icon_path

# URL：只有新URL不为空时才覆盖
if new_url.strip():
    merged['url'] = new_url
```

这样确保了：
- 🛡️ **手动修正的描述** - 如果新抓取失败，保留你的修正
- 🛡️ **手动修正的图标路径** - 如果新图标下载失败，保留原有路径
- 🛡️ **手动修正的其他字段** - 完全不会受到影响

### 📋 适用范围
**智能覆盖机制已应用到：**
- ✅ **怪物爬虫** (`selenium_monster_v3.py`) - 保护技能和物品数据
- ✅ **事件爬虫** (`selenium_event_final.py`) - 保护事件选择数据

### ✅ 四重保护
1. **自动备份** - 每次更新前创建带时间戳的完整备份
2. **增量更新** - 只添加新内容，不覆盖已有数据
3. **智能覆盖** - 只有新数据有效时才覆盖，保留手动修正
4. **手动修正保留** - 已有数据完全不动，100%保留你的修正

---

## 📖 详细步骤

### 步骤 1: 备份当前数据 ⭐ 必须！

```bash
cd "d:\PythonProject\BazaarInfo\bazaardb-desktop\6.0\crawlers"
python backup_current_data.py
```

**输出：**
- 创建 `backups/backup_YYYYMMDD_HHMMSS/` 目录
- 包含所有怪物、事件数据和图标
- 生成备份摘要文件

**检查：**
- 确认备份目录已创建
- 检查备份摘要，确保没有错误

---

### 步骤 2: 获取最新游戏数据

#### 2.1 抓取网页
```bash
python simple_crawler.py
```
生成 `debug_monsters.html` 和 `debug_events.html`

#### 2.2 提取怪物名称
```bash
python fetch_monster_name.py
```
生成 `unique_monsters.json`

#### 2.3 提取事件名称
```bash
python fetch_event_name.py
```
生成 `unique_events.json`

**⚠️ 重要：** 检查并手动修正 `unique_events.json`（如果需要）

---

### 步骤 3: 对比找出新增项

```bash
python compare_names.py
```

**功能：**
- 对比新旧名称列表
- 找出新增的怪物和事件
- 生成 `new_monsters.json` 和 `new_events.json`

**输出示例：**
```
[1/2] 对比怪物名称...
  总怪物数: 110
  已有怪物: 104
  新增怪物: 6

  新增的怪物:
    + New Monster 1
    + New Monster 2
    ...

[2/2] 对比事件名称...
  总事件数: 35
  已有事件: 30
  新增事件: 5

  新增的事件:
    + New Event 1
    + New Event 2
    ...
```

**检查：**
- 查看新增项是否合理
- 如果事件名称有误，手动修正 `new_events.json`

---

### 步骤 4: 爬取新增项

#### 方式 A: 自动合并（推荐，现有爬虫已支持）

爬虫会自动跳过已存在的项，只添加新项：

```bash
# 爬取怪物（会自动跳过已有的）
python selenium_monster_v3.py

# 爬取事件（会自动跳过已有的）
python selenium_event_final.py
```

**优点：**
- 简单，一条命令搞定
- 自动合并，不需要手动处理

**原理：**
- 爬虫在启动时会加载已有数据
- 检查每个名称是否已存在
- 只处理不存在的新项

---

#### 方式 B: 临时修改输入文件（更安全）

如果你担心自动跳过机制不可靠，可以临时替换输入文件：

```bash
# 1. 备份原名称列表
copy unique_monsters.json unique_monsters.json.backup
copy unique_events.json unique_events.json.backup

# 2. 用新增列表替换
copy new_monsters.json unique_monsters.json
copy new_events.json unique_events.json

# 3. 运行爬虫
python selenium_monster_v3.py
python selenium_event_final.py

# 4. 恢复原名称列表
copy unique_monsters.json.backup unique_monsters.json
copy unique_events.json.backup unique_events.json
```

---

### 步骤 5: 检查数据完整性

```bash
python check_data_completeness.py
```

**检查项：**
- 新增项是否全部爬取成功
- 图标是否下载完整
- 描述是否都存在

---

### 步骤 6: 手动处理失败项（如需要）

查看 `logs/error_log_*.json`，手动处理：
- 下载失败的图标
- 补充缺失的描述
- 修正错误的数据

---

### 步骤 7: 验证更新结果

```bash
# 查看最终统计
python check_data_completeness.py

# 对比更新前后的数量
# 检查 backup_summary.json vs 当前数据
```

---

## 🔧 工具脚本说明

### 1. backup_current_data.py
**功能：** 备份所有数据
**输出：** `backups/backup_YYYYMMDD_HHMMSS/`

### 2. compare_names.py
**功能：** 对比新旧名称，生成新增项列表
**输出：** `new_monsters.json`, `new_events.json`

### 3. merge_data.py
**功能：** 智能合并临时数据到正式数据（如果使用方式B）
**注意：** 方式A不需要此脚本

---

## 📊 数据保护机制详解

### 爬虫的增量保存机制

#### selenium_monster_v3.py
```python
def load_existing_monsters(output_file):
    """加载已处理的怪物数据（如果存在）"""
    if output_file.exists():
        return json.load(f)
    return []

# 在主函数中
all_monsters = load_existing_monsters(output_file)
existing_names = {m['name'] for m in all_monsters}
remaining_monsters = [n for n in monster_names if n not in existing_names]
```

**保护机制：**
- ✅ 启动时加载已有数据
- ✅ 自动跳过已处理的怪物
- ✅ 只处理剩余的怪物
- ✅ 每处理完一个立即保存

#### selenium_event_final.py
同样的机制应用于事件爬虫

---

## 🔄 完整工作流程图

```
┌─────────────────────────────────────────┐
│  1. 游戏更新了！                        │
└─────────────┬───────────────────────────┘
              ↓
┌─────────────────────────────────────────┐
│  2. 备份当前数据                        │
│     python backup_current_data.py       │
│  → backups/backup_YYYYMMDD_HHMMSS/      │
└─────────────┬───────────────────────────┘
              ↓
┌─────────────────────────────────────────┐
│  3. 获取最新名称列表                    │
│     python simple_crawler.py            │
│     python fetch_monster_name.py        │
│     python fetch_event_name.py          │
│  → unique_monsters.json                 │
│  → unique_events.json                   │
└─────────────┬───────────────────────────┘
              ↓
┌─────────────────────────────────────────┐
│  4. 手动修正事件名称（如需要）          │
│     编辑 unique_events.json             │
└─────────────┬───────────────────────────┘
              ↓
┌─────────────────────────────────────────┐
│  5. 对比找出新增项                      │
│     python compare_names.py             │
│  → new_monsters.json (6个新增)          │
│  → new_events.json (5个新增)            │
└─────────────┬───────────────────────────┘
              ↓
┌─────────────────────────────────────────┐
│  6. 爬取新增项                          │
│     python selenium_monster_v3.py       │
│     python selenium_event_final.py      │
│  → 自动跳过已有项，只爬新增            │
└─────────────┬───────────────────────────┘
              ↓
┌─────────────────────────────────────────┐
│  7. 检查数据完整性                      │
│     python check_data_completeness.py   │
└─────────────┬───────────────────────────┘
              ↓
┌─────────────────────────────────────────┐
│  8. 手动处理失败项（如有）              │
│     根据 error_log 补充                 │
└─────────────┬───────────────────────────┘
              ↓
┌─────────────────────────────────────────┐
│  9. 完成！手动修正完全保留              │
└─────────────────────────────────────────┘
```

---

## ⚠️ 重要提示

### 必须做的
1. ✅ **每次更新前先备份** - 使用 `backup_current_data.py`
2. ✅ **检查对比结果** - 运行 `compare_names.py` 后检查新增项
3. ✅ **验证更新结果** - 使用 `check_data_completeness.py`

### 不要做的
1. ❌ **不要手动删除已有数据** - 爬虫会自动跳过
2. ❌ **不要直接覆盖JSON文件** - 使用增量更新
3. ❌ **不要跳过备份步骤** - 万一出错可以恢复

---

## 🆘 问题排查

### Q1: 爬虫没有跳过已有项？
**A:** 检查名称是否完全匹配（包括大小写、空格）

### Q2: 新增项没有被识别？
**A:** 运行 `compare_names.py` 查看详细对比结果

### Q3: 手动修正的内容被覆盖了？
**A:** 
1. 立即停止爬虫
2. 从 `backups/` 恢复备份
3. 检查爬虫是否正确加载了已有数据

### Q4: 如何恢复备份？
**A:**
```bash
# 进入备份目录
cd backups/backup_YYYYMMDD_HHMMSS/

# 复制回原位置
xcopy monster_details_v3 ..\..\monster_details_v3 /E /Y
xcopy event_details_final ..\..\event_details_final /E /Y
```

---

## 📞 需要帮助？

如果遇到问题：
1. 查看 `logs/error_log_*.json` 了解详情
2. 检查备份是否完整
3. 参考本指南的问题排查部分

---

**最后更新：** 2025-10-10
**版本：** v6.0

